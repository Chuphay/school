{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From wikipedia:\n",
    "\n",
    "Non-negative matrix factorization (NMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/\n",
    "\n",
    " Firstly, we have a set U of users, and a set D of items. Let $\\mathbf{R}$ of size $|U| \\times |D|$ be the matrix that contains all the ratings that the users have assigned to the items. Also, we assume that we would like to discover $K$ latent features. Our task, then, is to find two matrics matrices $ \\mathbf{P} (a |U| \\times K matrix)$ and $\\mathbf{Q} (a |D| \\times K matrix)$ such that their product approximates $\\mathbf{R}$:\n",
    "\n",
    "$$\\mathbf{R} \\approx \\mathbf{P} \\times \\mathbf{Q}^T = \\hat{\\mathbf{R}}$$\n",
    "\n",
    "In this way, each row of $\\mathbf{P}$ would represent the strength of the associations between a user and the features. Similarly, each row of $\\mathbf{Q}$ would represent the strength of the associations between an item and the features. To get the prediction of a rating of an item $d_j$ by $u_i$, we can calculate the dot product of the two vectors corresponding to $u_i$ and $d_j$:\n",
    "\n",
    "$$\\hat{r}_{ij} = p_i^T q_j = \\sum_{k=1}^k{p_{ik}q_{kj}}$$\n",
    "\n",
    "Now, we have to find a way to obtain $\\mathbf{P}$ and $\\mathbf{Q}$. One way to approach this problem is the first intialize the two matrices with some values, calculate how \"different\" their product is to $\\mathbf{R}$, and then try to minimize this difference iteratively. Such a method is called gradient descent, aiming at finding a local minimum of the difference.\n",
    "\n",
    "The difference here, usually called the error between the estimated rating and the real rating, can be calculated by the following equation for each user-item pair:\n",
    "\n",
    "\n",
    "$$e_{ij}^2 = (r_{ij} - \\hat{r}_{ij})^2 = (r_{ij} - \\sum_{k=1}^K{p_{ik}q_{kj}})^2$$\n",
    "\n",
    "Here we consider the squared error because the estimated rating can be either higher or lower than the real rating.\n",
    "\n",
    "To minimize the error, we have to know in which direction we have to modify the values of $p_{ik}$ and $q_{kj}$. In other words, we need to know the gradient at the current values, and therefore we differentiate the above equation with respect to these two variables separately:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial p_{ik}}e_{ij}^2 = -2(r_{ij} - \\hat{r}_{ij})(q_{kj}) = -2 e_{ij} q_{kj}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial q_{ik}}e_{ij}^2 = -2(r_{ij} - \\hat{r}_{ij})(p_{ik}) = -2 e_{ij} p_{ik}$$\n",
    "\n",
    "Having obtained the gradient, we can now formulate the update rules for both $p_{ik}$ and $q_{kj}$:\n",
    "\n",
    "$$p'_{ik} = p_{ik} + \\alpha \\frac{\\partial}{\\partial p_{ik}}e_{ij}^2 = p_{ik} + 2\\alpha e_{ij} q_{kj}$$\n",
    "\n",
    "$$q'_{kj} = q_{kj} + \\alpha \\frac{\\partial}{\\partial q_{kj}}e_{ij}^2 = q_{kj} + 2\\alpha e_{ij} p_{ik}$$ \n",
    "\n",
    "Here, $\\alpha$ is a constant whose value determines the rate of approaching the minimum. Usually we will choose a small value for $\\alpha$, say 0.0002. This is because if we make too large a step towards the minimum we may run into the risk of missing the minimum and end up oscillating around the minimum.\n",
    "\n",
    "Using the above update rules, we can then iteratively perform the operation until the error converges to its minimum. We can check the overall error as calculated using the following equation and determine when we should stop the process.\n",
    "\n",
    "$$E = \\sum_{(u_i,d_j,r_{ij}) \\in T}{e_{ij}} = \\sum_{(u_i,d_j,r_{ij}) \\in T}{(r_{ij} - \\sum_{k=1}^K{p_{ik}q_{kj}})^2}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broken\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.96277256,  0.63848161,  0.30044333,  0.66122009],\n",
       "       [ 0.6538398 ,  0.64671876,  0.65231356,  1.00173622],\n",
       "       [ 1.01973051,  1.95412775,  3.00619344,  4.01439045]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matrix_factorization(R, k, alpha = 0.001, tol = 0.1):\n",
    "    n,m = R.shape\n",
    "    p = randn(n,k)\n",
    "    q = randn(k,m)\n",
    "    breaker = 1000\n",
    "    while True:\n",
    "        if(breaker <= 0):\n",
    "            print(\"broken\")\n",
    "            break\n",
    "        breaker -= 1    \n",
    "        error = (R - dot(p,q))\n",
    "        if (sum(error**2) < tol) :\n",
    "            break\n",
    "        p += alpha*dot(error,q.T)\n",
    "        q += alpha*dot(error.T, p).T\n",
    "    return (p,q)    \n",
    "    \n",
    "p,q = matrix_factorization(array([[1,1,1,0],[1,0,1,1],[1,2,3,4]]),2) \n",
    "dot(p,q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big major problem with this is that, we are certainly missing the non-negative aspect of this...\n",
    "\n",
    "from: http://epubs.siam.org/doi/pdf/10.1137/1.9781611972757.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
