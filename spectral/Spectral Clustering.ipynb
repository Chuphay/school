{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Spectral Clustering\n",
    "\n",
    "Without getting too much into the details. In order to cluster the graph into two clusters, we use the second smallest eigenvalue. \n",
    "\n",
    "As we saw in the Laplacian and its eigenvalues notebook, the smallest eigenvalue of the Laplacian is always zero. This corresponds to an eigenvalue proportional to a vector of all constant values.\n",
    "\n",
    "The second smallest eigenvalue, will have an eigenvector that roughly splits the vertices of the graph, with half of the vertices corresponding to positive values in the eigenvector and half of the vertices corresponding to negative values. This makes it easy to form two clusters.\n",
    "\n",
    "Since we are only looking for one eigenvalue (the second lowest one) and its corresponding eigenvector. The QR method, presented before, seems a bit too over-blown. In this notebook, we will examine how to find single eigenvectors and their eigenvalues.\n",
    "\n",
    "This can be accomplished by using various versions of what has been called the \"power method\". As before, because I used a variety of sources, I have a variety of different algorithms that can be used in a variety of situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Hoffman's Power Method\n",
    "\n",
    "\n",
    "1. Assume a trial value $\\mathbf{x}^{(0)}$. Choose one component to be unity. Designate that the unity component.\n",
    "\n",
    "2. Perform the matrix multiplication: $\\mathbf{A}\\mathbf{x}^{(0)} = \\mathbf{y}^{(1)}$\n",
    "\n",
    "3. Scale $\\mathbf{y}^{(1)}$ so that the unity component remains unity:\n",
    "\n",
    "$$\\mathbf{y}^{(1)} = \\lambda^{(1)} \\mathbf{x}^{(1)}$$\n",
    "\n",
    "4. Repeat: \n",
    "\n",
    "$$\\mathbf{A}\\mathbf{x}^{(k)} = \\mathbf{y}^{(k+1)} = \\lambda^{(k+1)} \\mathbf{x}^{(k+1)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13.870603889183611,\n",
       " array([ 1.        ,  0.49178084, -3.42707923]),\n",
       " 1.1429145457597656e-05)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hoffman_power(A):\n",
    "    n = A.shape[0]\n",
    "    x = ones(n) #note, this dies on the laplacian, see below\n",
    "    lmda = 0\n",
    "    for i in range(30):\n",
    "        last = lmda\n",
    "        y = dot(A,x)\n",
    "        lmda = y[0]\n",
    "        x = y/lmda\n",
    "    error = last - lmda    \n",
    "    return (lmda, x, abs(error))    \n",
    "    \n",
    "\n",
    "A = array([[8,-2,-2],[-2,4,-2],[-2,-2,13]])\n",
    "hoffman_power(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The method, just presented finds the biggest eigenvalue. What will often be of interest (although not for clustering, but included here for completeness) is finding the smallest eigenvalue. We do this with the inverse power method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Inverse Power Method\n",
    "\n",
    "1. Solve for $\\mathbf{L}$ and $\\mathbf U$ such that $\\mathbf{LU} = \\mathbf{A}$\n",
    "\n",
    "2. Assume $\\mathbf{x}^{(0)}$. Designate a component of $\\mathbf{x}$ to be unity.\n",
    "\n",
    "3. Solve for $\\mathbf{x}'$ by forward substitution: $\\mathbf{L} \\mathbf{x}' = \\mathbf{x}^{(0)}$\n",
    "\n",
    "4. Solve for $\\mathbf{y}^{(1)}$ by back substitution: $\\mathbf{U}\\mathbf{y}^{(1)} =  \\mathbf{x}' $\n",
    "\n",
    "5. Scale $\\mathbf{y}^{(1)}$ so that the unity component is unity. Thus, $\\mathbf{y}^{(1)} = \\lambda_{\\text{inverse}}^{(1)} \\mathbf{x}^{(1)} $\n",
    "\n",
    "6. Repeat\n",
    "\n",
    "$$\\mathbf{L} \\mathbf{x}' = \\mathbf{x}^{(k)}$$\n",
    "\n",
    "$$ \\mathbf{U}\\mathbf{y}^{(k+1)} =  \\mathbf{x}'$$\n",
    "\n",
    "$$ \\mathbf{y}^{(k+1)} = \\lambda_{\\text{inverse}}^{(k+1)} \\mathbf{x}^{(k+1)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5090028139770366,\n",
       " array([ 1.        ,  2.14578754,  0.59971106]),\n",
       " 8.4845877590389307e-06)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.linalg import lu\n",
    "\n",
    "\n",
    "def hoffman_inverse_power(A):\n",
    "    l,u = lu(A, permute_l=True)\n",
    "    n = A.shape[0]\n",
    "    x = ones(n)\n",
    "    x_prime = ones(n)\n",
    "    y = ones(n)\n",
    "    lmda = 0\n",
    "    \n",
    "    for i in range(10):\n",
    "        last = lmda\n",
    "        for j in range(n):\n",
    "            x_prime[j] = x[j]/l[j][j]\n",
    "            for k in range(j):\n",
    "                x_prime[j] -= x_prime[k]*l[j][k]\n",
    "\n",
    "        for j in reversed(range(n)):\n",
    "            y[j] = x_prime[j]/u[j][j]\n",
    "            for k in range(j+1, n):\n",
    "                y[j] -= y[k]*u[j][k]/u[j][j]\n",
    "\n",
    "        lmda = y[0] \n",
    "        x = y/lmda\n",
    "\n",
    "    return (1/lmda, x, abs(lmda-last))    \n",
    "    \n",
    "A = array([[8,-2,-2],[-2,4,-2],[-2,-2,13]])\n",
    "hoffman_inverse_power(A)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Shifted Power Method\n",
    "\n",
    "The eigenvalues of a matrix $\\mathbf{A}$ may be shifted by a scalar s by subtracting $s\\mathbb{I} \\mathbf{x} = s \\mathbf{x}$ from both sides of the standard eigenproblem, $\\mathbf{A} \\mathbf{x} = \\lambda \\mathbf{x} $. Thus,\n",
    "\n",
    "$$\\mathbf{A} \\mathbf{x}-s\\mathbb{I}\\mathbf{x}  = \\lambda \\mathbf{x} -s \\mathbf{x}$$\n",
    "\n",
    "$$(\\mathbf{A} - s\\mathbb{I})\\mathbf{x}  = (\\lambda -s )\\mathbf{x}$$\n",
    "\n",
    "$$\\mathbf{A}_{\\text{shifted}} \\mathbf{x} = \\lambda_{\\text{shifted}} \\mathbf{x}$$\n",
    "\n",
    "Shifting the eigenvalues of a matrix can be used to:\n",
    "\n",
    "1. Find the opposite extreme eigenvalue.\n",
    "\n",
    "2. Find intermediate eigenvalues\n",
    "\n",
    "3. Accelerate convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.508980800011047"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = array([[8,-2,-2],[-2,4,-2],[-2,-2,13]])\n",
    "value, vector, error = hoffman_power(A)\n",
    "A_shifted = A - eye(3)*value\n",
    "value2, vector, error = hoffman_power(A_shifted)\n",
    "value2 +value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Direct Method\n",
    "\n",
    "The characteristic equation is obtained from\n",
    "\n",
    "$$\\text{det}(\\mathbf{A} - \\lambda \\mathbb{I}) = 0$$\n",
    "\n",
    "Rather than solving for the roots of the characteristic equation directly, we instead solve the characteristic equation iteratively (my sentences). \"This can be accomplished my applying the secant method. Two intitial approximations of $\\lambda$ are assumed, $\\lambda_0$ and $\\lambda_1$, the corresponding values of the cahracteristic determinant are computed, and these results are used to construct a linear relationship between $\\lambda$ and the value of the characteristic determinant. The solution of that linear relationship is taken as the next approximation to $\\lambda$, and the procedure is repeated to convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.870585123309459"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def direct_method(A, guess_1, guess_2):\n",
    "    if(abs(guess_2 - guess_1)< 1e-5):\n",
    "        return guess_2\n",
    "    else:\n",
    "        n = A.shape[0]\n",
    "        f_1 = det(A - eye(n)*guess_1)\n",
    "        f_2 = det(A - eye(n)*guess_2)\n",
    "        slope = (f_2 - f_1)/(guess_2 - guess_1)\n",
    "        guess_3 = guess_2 - f_2/slope\n",
    "        return direct_method(A, guess_2, guess_3)\n",
    "    \n",
    "    \n",
    "direct_method(A,15, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From http://www.math.tamu.edu/~dallen/linear_algebra/chpt5.pdf\n",
    "\n",
    "###Finding Nondominant Eigenvalues\n",
    "\n",
    "Once the dominant eigenpair $(\\lambda_1, V_1)$ of $A$ has been computed, we may wish to compute $\\lambda_2$. If $A$ is symmetric, it can be proved that if $U_1 = V_1 / |V_1|$, then\n",
    "\n",
    "$$A^{(2)} = A - \\lambda_1 U_1 U_1^T$$\n",
    "\n",
    "has eigenvalues $0, \\lambda_2, \\lambda_3, ..., \\lambda_n$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.66449861e-17   4.38447187e-01   3.00000000e+00   3.00000000e+00\n",
      "   3.00000000e+00   4.56155281e+00]\n",
      "4.56155067632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43844718719117015"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = array([[2,-1,-1,0,0,0],[-1,2,-1,0,0,0],[-1,-1,3,-1,0,0],[0,0,-1,3,-1,-1],[0,0,0,-1,2,-1],[0,0,0,-1,-1,2]])\n",
    "print(sort(eig(A)[0]))\n",
    "\n",
    "def hoffman_power(A):\n",
    "    n = A.shape[0]\n",
    "    x = zeros(n)\n",
    "    x[0] = 1\n",
    "    lmda = 0\n",
    "    for i in range(40):\n",
    "        last = lmda\n",
    "        y = dot(A,x)\n",
    "        lmda = y[0]\n",
    "        x = y/lmda\n",
    "    error = last - lmda    \n",
    "    return (lmda, x, abs(error))    \n",
    "\n",
    "max_e_value = hoffman_power(A)[0]\n",
    "print(max_e_value)\n",
    "\n",
    "#now we shift\n",
    "A_1 = A - max_e_value*eye(6)\n",
    "\n",
    "#and then, we know that there is an eigenvalue with -max_e_value and all 1's eigenvector\n",
    "A_2 = A_1 + max_e_value*outer(ones(6),ones(6))/6\n",
    "\n",
    "#and then we can calculate the second smallest eigenvalue easily: \n",
    "hoffman_power(A_2)[0]+max_e_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.438447187191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.46470513,  0.46470513,  0.26095647, -0.26095647, -0.46470513,\n",
       "       -0.46470513])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e,v = eig(A)\n",
    "print(e[3])\n",
    "v[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4647    ,  0.4647    ,  0.26095359, -0.26095359, -0.4647    ,\n",
       "       -0.4647    ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoffman_power(A_2)[1]*0.4647"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And finally, we see that we have successfully clustered the nodes into positive and negative nodes which constitute a cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
