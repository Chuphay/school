{
 "metadata": {
  "name": "",
  "signature": "sha256:21a1dde54c12d208a47a5d6126a8aa0eba809df5a021ec383e01adca9d3b6633"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Graduate students are required to complete a research or design project in AI on a topic to be chosen in consultation with me. A written report on the project (minimum 2,500 words) and a brief oral presentation summarizing the same is expected at the end of the term. An ideal project should be one that demonstrates some creativity, attempts to answer some interesting research question(s), or offers an interesting AI solution to a problem of practical interest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = \"\"\"Creativity\n",
      "My project demonstrates creativity. I created an atificial neural network from scratch, implementing the details in C. While originally I had bold plans for my Artificial Neural Network, I was crushed by the daunting task, both because of the simple fact that the algorithms involved are highly complex and difficult to understand, but also because I chose to implement the program in C, a language that I have exposure to, but is also not my most comfortable language.\n",
      "My original plan had been to make my artificial Neural network be able to learn how to calculate floating point additions. On the surface, perhaps, this seems like a trivial task, as any modern computer can do floating point calculations. But in fact, this is quite an interesting and creative project for a nueral network to accomplish. As we all know, computers work in a world of ones and zeroes. Floating point calculations by their natures truncate, or approxiamte their answers. On top of that, as we will see in this paper, most beginning applications of Artificial Neural networks concentrate on simple boolean functions, and I thought it would have been interesting to try and up the game a bit. However, as we will also see, I couldn't do that and had to eventually drop back down to the simple boolean functions.\n",
      "The second reason why the floating point calculation is cool, besides for the fact that by their natures ANNs simply output a string of ones and zeros, is because the ANN is truelly learning how to do these calculations. This is like teaching a child how to multiply, but not only teaching him or her how to multiply single digit numbers, but arbitrary floating point numbers (actually in the system I had originally conceived, there still would have been limits on exactly how large the numbers could be). The way you teach a child to multiply is basically the same way you teach an ANN: practice, practice, practice.\n",
      "Interesting Research Questions\n",
      "(maybe this whole section is best at the very end)\n",
      "Unfurtunately the timing in the quarter system does not allow us to fully dive into a project. I would approximate that I spent something like 5 weeks programming the basic algorithms. Now that this is done, there are many questions that can be asked.\n",
      "Questions of topology. (at some point we should introduce what an ANN is)\n",
      "Questions of learning methods: one exampl at a time, batch, sleep, etc.\n",
      "Questions of memory: how many rules can an ANN of a certain topology handle. As one of my papers stated. For simple boolean, you don't need a hidden layer. For more complicated boolean operators, you will need to have a hidden layer. One hidden layer also works for addition. However, to multiply you will need to have two hidden layers. But on top of this is the memory we are already familiar with, where we can put information into short term memory for later and fast retrieval.\n",
      "chaining inputs and outputs so that's it can handle digits greater than one byte. I haven't figured out how to do this yet.\n",
      "sigmoid function dopeness. you have to do discrete calculus rather than normal calculus. It was easiest to implentmeant this as floating point, so for expediances sake I have not worked on this further.\n",
      "C versus Python\n",
      "C is an interesting choice because I can now, and as we will see in the future, I can get very close to the machine in order to get fast performance using a minimum of space. I did not choose Python because I wanted to implement a really efficient system. In retrospect this was probably a mistake. Given the complexity of the task, Python would have been a better choice for at least two reasons. One, Python has a way of making the complex seem, if not trivial, at least tackleble. Two, because of the aforementioned complexity, I was not actually able to implement the program in a clean and precise way, and in fact had to not even think about garbage collection and other issues of equal or perhaps more importance, as I will explain in a different section\n",
      "description of an ANN\n",
      "Artificial Neural Networks or ANNs is an old subfield of Arificial Intelligence research that has recently been resurgent. ANNs attempt to mimic the actual biological mechanisms of a brain. It began in 1943 when Warren McCulloch and Walter Pitts published a paper on them. The human brain is made up of billions of nuerons. Neurons are connected to each other by synapses. Each neuron has hundreds if not thousands of synapses as inputs and thence their output is also connected to thousands of synapses. And in theory, the neuron fires when certain stimuli from the various inputs reaches a certain threshold. The computer model tries to copy this behavior. Every neuron will have a certain number of inputs, and if a certain criterion is reached, then the neuron will fire and send its output to other neurons in the net.\n",
      "Rather than talking about all the steps necessary to create an ANN from the beginning. I am going to walk you hrough all the steps and design decisions that I made while implementing my Neural network. First, I will talk about simple perceptrons (I believe that's what they are called) and then...\n",
      "\n",
      " \"\"\"\n",
      "len(a.split())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "893"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<u>Creativity</u>\n",
      "\n",
      "My project demonstrates creativity. I created an atificial neural network from scratch, implementing the details in C. While originally I had bold plans for my Artificial Neural Network, I was crushed by the daunting task, both because of the simple fact that the algorithms involved are highly complex and difficult to understand, but also because I chose to implement the program in C, a language that I have exposure to, but is also not my most comfortable language. \n",
      "\n",
      "My original plan had been to make my artificial Neural network be able to learn how to calculate floating point additions. On the surface, perhaps, this seems like a trivial task, as any modern computer can do floating point calculations. But in fact, this is quite an interesting and creative project for a nueral network to accomplish. As we all know, computers work in a world of ones and zeroes. Floating point calculations by their natures truncate, or approxiamte their answers. On top of that, as we will see in this paper, most beginning applications of Artificial Neural networks concentrate on simple boolean functions, and I thought it would have been interesting to try and up the game a bit. However, as we will also see, I couldn't do that and had to eventually drop back down to the simple boolean functions.\n",
      "\n",
      "The second reason why the floating point calculation is cool, besides for the fact that by their natures ANNs simply output a string of ones and zeros, is because the ANN is truelly learning how to do these calculations. This is like teaching a child how to multiply, but not only teaching him or her how to multiply single digit numbers, but arbitrary floating point numbers (actually in the system I had originally conceived, there still would have been limits on exactly how large the numbers could be). The way you teach a child to multiply is basically the same way you teach an ANN: practice, practice, practice.\n",
      "\n",
      "<u>Interesting Research Questions</u>\n",
      "\n",
      "(maybe this whole section is best at the very end)\n",
      "\n",
      "Unfurtunately the timing in the quarter system does not allow us to fully dive into a project. I would approximate that I spent something like 5 weeks programming the basic algorithms. Now that this is done, there are many questions that can be asked. \n",
      "\n",
      "Questions of topology. (at some point we should introduce what an ANN is)\n",
      "\n",
      "Questions of learning methods: one exampl at a time, batch, sleep, etc.\n",
      "\n",
      "Questions of memory: how many rules can an ANN of a certain topology handle. As one of my papers stated. For simple boolean, you don't need a hidden layer. For more complicated boolean operators, you will need to have a hidden layer. One hidden layer also works for addition. However, to multiply you will need to have two hidden layers. But on top of this is the memory we are already familiar with, where we can put information into short term memory for later and fast retrieval.\n",
      "\n",
      "chaining inputs and outputs so that's it can handle digits greater than one byte. I haven't figured out how to do this yet.\n",
      "\n",
      "\n",
      "sigmoid function dopeness. you have to do discrete calculus rather than normal calculus. It was easiest to implentmeant this as floating point, so for expediances sake I have not worked on this further. \n",
      "\n",
      "\n",
      "<u>C versus Python</u>\n",
      "\n",
      "C is an interesting choice because I can now, and as we will see in the future, I can get very close to the machine in order to get fast performance using a minimum of space. I did not choose Python because I wanted to implement a really efficient system. In retrospect this was probably a mistake. Given the complexity of the task, Python would have been a better choice for at least two reasons. One, Python has a way of making the complex seem, if not trivial, at least tackleble. Two, because of the aforementioned complexity, I was not actually able to implement the program in a clean and precise way, and in fact had to not even think about garbage collection and other issues of equal or perhaps more importance, as I will explain in a different section\n",
      "\n",
      "<u>description of an ANN</u>\n",
      "\n",
      "Artificial Neural Networks or ANNs is an old subfield of Arificial Intelligence research that has recently been resurgent. ANNs attempt to mimic the actual biological mechanisms of a brain. It began in 1943 when Warren McCulloch and Walter Pitts published a paper on them. The human brain is made up of billions of nuerons. Neurons are connected to each other by synapses. Each neuron has hundreds if not thousands of synapses as inputs and thence their output is also connected to thousands of synapses. And in theory, the neuron fires when certain stimuli from the various inputs reaches a certain threshold. The computer model tries to copy this behavior. Every neuron will have a certain number of inputs, and if a certain criterion is reached, then the neuron will fire and send its output to other neurons in the net. \n",
      "\n",
      "Rather than talking about all the steps necessary to create an ANN from the beginning. I am going to walk you hrough all the steps and design decisions that I made while implementing my Neural network. First, I will talk about simple perceptrons (I believe that's what they are called) and then..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<u>Walkthrough of my code</u>\n",
      "\n",
      "First up, really before anything else is to talk about the good things about C. The really nice thing is how close it lets me get to the machine, and I actually operate at the individual level of bits and bytes. Unfourtunately, or really this is just the way that C is made, I can actually only operate at the level of Bytes. In order to manipulate the individual bits, I have to use 'masks'. Here below is a little procedure to ptrint the individual bits in a byte."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "#define BYTE 8\n",
      "\n",
      "void print_byte(char a){\n",
      "  int i;\n",
      "  for(i = 0; i < BYTE; i++) {\n",
      "      printf(\"%d\", !!((a << i) & 128));\n",
      "  }\n",
      "  printf(\"\\n\");\n",
      "}\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the get go, it should be pointed out that since we are going to consentrate our efforts on one byte at a time, we will be restricetd to positive numbers less than 256. To deal with bigger numbers we will have to do some type of chaining, I have not implemented this yet. \n",
      "\n",
      "My first efforts were directed towards the earliest research on ANNs and simply trying to express simple boolean functions like AND and OR. While later on, we wish to have the weights of the synapses vary according to their importance in a learned way, here, I was satisfied with hard-wiring the correct behaviour.\n",
      "\n",
      "Let's take a close look at how an AND perceptron would behave. There are two binary inputs into the perceptron, plus there is an ever present bias unit. This bias unit is sort of like a door or a gate, and only when the added weight of all the inputs bypass the weight of the door, does the perceptron fire. We are here assuming a hard cut-off (I will talk about this a little more momentarily). The bias unit is always set to one.\n",
      "\n",
      "$$if ~~~ \\sum {inputs * weights } \\ge 0 ~~~ then ~~FIRE!$$\n",
      "\n",
      "With two inputs and one bias unit, we need to have three weights. here are the weights that will make an AND perceptron work: 2 , 2, -3. Let's take a look at how that works. \n",
      "\n",
      "If both inputs are zero, then only the bias unit will contribute, and will contribute $0*2+0*2+1*(-3) = -3$ and therefore the perceptron will not fire. If the first input is a one, but the second input is zero, then the total contribution is $1*2+0*2+1*(-3)= -1$ and again the perceptron will not fire. If the first input is zero and the second input is one, the situation is almost exactly the same. Finally if both inputs are one we have  $1*2+1*2+1*(-3) = 1$ and finally the perceptron will fire, matching the exact performance of an AND gate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}