{
 "metadata": {
  "name": "",
  "signature": "sha256:5628f21d759b36759e5df871af820176fa479a863306bcaedaf5b25cf9601a85"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Graduate students are required to complete a research or design project in AI on a topic to be chosen in consultation with me. A written report on the project (minimum 2,500 words) and a brief oral presentation summarizing the same is expected at the end of the term. An ideal project should be one that demonstrates some creativity, attempts to answer some interesting research question(s), or offers an interesting AI solution to a problem of practical interest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = \"\"\"Creativity\n",
      "My project demonstrates creativity. I created an atificial neural network from scratch, implementing the details in C. While originally I had bold plans for my Artificial Neural Network, I was crushed by the daunting task, both because of the simple fact that the algorithms involved are highly complex and difficult to understand, but also because I chose to implement the program in C, a language that I have exposure to, but is also not my most comfortable language.\n",
      "My original plan had been to make my artificial Neural network be able to learn how to calculate floating point additions. On the surface, perhaps, this seems like a trivial task, as any modern computer can do floating point calculations. But in fact, this is quite an interesting and creative project for a nueral network to accomplish. As we all know, computers work in a world of ones and zeroes. Floating point calculations by their natures truncate, or approxiamte their answers. On top of that, as we will see in this paper, most beginning applications of Artificial Neural networks concentrate on simple boolean functions, and I thought it would have been interesting to try and up the game a bit. However, as we will also see, I couldn't do that and had to eventually drop back down to the simple boolean functions.\n",
      "The second reason why the floating point calculation is cool, besides for the fact that by their natures ANNs simply output a string of ones and zeros, is because the ANN is truelly learning how to do these calculations. This is like teaching a child how to multiply, but not only teaching him or her how to multiply single digit numbers, but arbitrary floating point numbers (actually in the system I had originally conceived, there still would have been limits on exactly how large the numbers could be). The way you teach a child to multiply is basically the same way you teach an ANN: practice, practice, practice.\n",
      "Interesting Research Questions\n",
      "(maybe this whole section is best at the very end)\n",
      "Unfurtunately the timing in the quarter system does not allow us to fully dive into a project. I would approximate that I spent something like 5 weeks programming the basic algorithms. Now that this is done, there are many questions that can be asked.\n",
      "Questions of topology. (at some point we should introduce what an ANN is)\n",
      "Questions of learning methods: one exampl at a time, batch, sleep, etc.\n",
      "Questions of memory: how many rules can an ANN of a certain topology handle. As one of my papers stated. For simple boolean, you don't need a hidden layer. For more complicated boolean operators, you will need to have a hidden layer. One hidden layer also works for addition. However, to multiply you will need to have two hidden layers. But on top of this is the memory we are already familiar with, where we can put information into short term memory for later and fast retrieval.\n",
      "chaining inputs and outputs so that's it can handle digits greater than one byte. I haven't figured out how to do this yet.\n",
      "sigmoid function dopeness. you have to do discrete calculus rather than normal calculus. It was easiest to implentmeant this as floating point, so for expediances sake I have not worked on this further.\n",
      "C versus Python\n",
      "C is an interesting choice because I can now, and as we will see in the future, I can get very close to the machine in order to get fast performance using a minimum of space. I did not choose Python because I wanted to implement a really efficient system. In retrospect this was probably a mistake. Given the complexity of the task, Python would have been a better choice for at least two reasons. One, Python has a way of making the complex seem, if not trivial, at least tackleble. Two, because of the aforementioned complexity, I was not actually able to implement the program in a clean and precise way, and in fact had to not even think about garbage collection and other issues of equal or perhaps more importance, as I will explain in a different section\n",
      "description of an ANN\n",
      "Artificial Neural Networks or ANNs is an old subfield of Arificial Intelligence research that has recently been resurgent. ANNs attempt to mimic the actual biological mechanisms of a brain. It began in 1943 when Warren McCulloch and Walter Pitts published a paper on them. The human brain is made up of billions of nuerons. Neurons are connected to each other by synapses. Each neuron has hundreds if not thousands of synapses as inputs and thence their output is also connected to thousands of synapses. And in theory, the neuron fires when certain stimuli from the various inputs reaches a certain threshold. The computer model tries to copy this behavior. Every neuron will have a certain number of inputs, and if a certain criterion is reached, then the neuron will fire and send its output to other neurons in the net.\n",
      "Rather than talking about all the steps necessary to create an ANN from the beginning. I am going to walk you hrough all the steps and design decisions that I made while implementing my Neural network. First, I will talk about simple perceptrons (I believe that's what they are called) and then...\n",
      "Walkthrough of my code\n",
      "First Version\n",
      "First up, really before anything else is to talk about the good things about C. The really nice thing is how close it lets me get to the machine, and I actually operate at the individual level of bits and bytes. Unfourtunately, or really this is just the way that C is made, I can actually only operate at the level of Bytes. In order to manipulate the individual bits, I have to use 'masks'. Here below is a little procedure to ptrint the individual bits in a byte.\n",
      "#define BYTE 8\n",
      "\n",
      "void print_byte(char a){\n",
      "  int i;\n",
      "  for(i = 0; i < BYTE; i++) {\n",
      "      printf(\"%d\", !!((a << i) & 128));\n",
      "  }\n",
      "  printf(\"\\n\");\n",
      "}\n",
      "From the get go, it should be pointed out that since we are going to consentrate our efforts on one byte at a time, we will be restricetd to positive numbers less than 256. To deal with bigger numbers we will have to do some type of chaining, I have not implemented this yet.\n",
      "My first efforts were directed towards the earliest research on ANNs and simply trying to express simple boolean functions like AND and OR. While later on, we wish to have the weights of the synapses vary according to their importance in a learned way, here, I was satisfied with hard-wiring the correct behaviour.\n",
      "Let's take a close look at how an AND perceptron would behave. There are two binary inputs into the perceptron, plus there is an ever present bias unit. This bias unit is sort of like a door or a gate, and only when the added weight of all the inputs bypass the weight of the door, does the perceptron fire. We are here assuming a hard cut-off (I will talk about this a little more momentarily). The bias unit is always set to one.\n",
      "if   \u2211inputs\u2217weights\u22650   then  FIRE!\n",
      "With two inputs and one bias unit, we need to have three weights. here are the weights that will make an AND perceptron work: 2 , 2, -3. Let's take a look at how that works.\n",
      "If both inputs are zero, then only the bias unit will contribute, and will contribute 0\u22172+0\u22172+1\u2217(\u22123)=\u22123 and therefore the perceptron will not fire. If the first input is a one, but the second input is zero, then the total contribution is 1\u22172+0\u22172+1\u2217(\u22123)=\u22121 and again the perceptron will not fire. If the first input is zero and the second input is one, the situation is almost exactly the same. Finally if both inputs are one we have 1\u22172+1\u22172+1\u2217(\u22123)=1 and finally the perceptron will fire, matching the exact performance of an AND gate.\n",
      "If you have never done so, I recommend trying to figure out the weights for an OR perceptron yourself.\n",
      "If you try to make the weghts for an XOR gate, you will quickly find that it is impossible. And that you will need to chain several perceptrons together. Before getting to this, however, I want to discuss my implementation of the AND gate in C, as I found that at every step in this journey led to decisions that implications for the life of the project.\n",
      "Obviously, in the beginning, and having played with someone else's python ANN. I was expecting that I could have arbitrary topologies or architectures. But my abilities with C would not allow that to happen.\n",
      "At this point in the project, I was still hoping to be able to do floating point arithmetic, so I needed to train my network to accept input much larger than just two bits plus a bias unit. At the time I hadn't set in stone that this would be my final architecture, but as I progressed, I realized that I would need to make decisions that would... and this was a decision that I made that stuck. What I decided for this is that I would accept a byte for each input and I would accept a final control byte, which would operate as my bias unit, but could be used in the future for some extra flexibility (I have not been able to take advantage of this.). I also decide to have only one byteo for the output (I now regret this, and would have much preferred haveing two bytes of output). This means that my perceptron would accept 24 bits of information and would output 8 bits. This makes for a total of 192 weights internally in each neuron or perceptron.\n",
      "Before dealing with that, we really must talk about the hard or soft squashing function. There are a few standard models. In the above example we used a hard squahing decision thing. But usually something with a softer edge has an advantage. The most common choice is the sigmoid or logistic function. I have actually had considerable dealings with this function in the past, but I will not burden the reader with that here and now. The sigmoid function is defined as\n",
      "\u03d5(x)=11+e\u2212x\n",
      "Also, the derivative of this has the extremely simple form: \u03d5\u2217(1\u2212\u03d5) which will come into play when we discuss backpropagation.\n",
      "As I was saying we must sum over all the inputs multiplied by their weights. Two Bytes of input plus one byte for the control bit, gives us 24 bits to multiply together. I created a simple function for both of these two procedures:\n",
      "#define SIZE 3*BYTE\n",
      "\n",
      "double sigmoid(double z){\n",
      "  if(z>4) return 1;\n",
      "  if(z<-4) return 0;\n",
      "\n",
      "  double denom = 1 + exp(-z);\n",
      "  return 1.0/denom;\n",
      "} \n",
      "\n",
      "double dot(double *a, double *b){\n",
      "  double out = 0;\n",
      "  int i;\n",
      "  for(i = 0; i < SIZE; i++){\n",
      "    out += a[i]*b[i];\n",
      "  }\n",
      "  return out;\n",
      "}\n",
      "Setting up the AND neuron was fairly strightforward. The only crucial piece of technology was that I wanted to ensure that the control bit did indeed have a leading one, otherwise it couldn't be used as a bias. I did this witha simple mask:\n",
      "a = a|128\n",
      "128 being the byte that has a leading 1 with the rest of the digits zero. I then initialized a matrix:\n",
      "double theta[BYTE][SIZE] = {};\n",
      "and intitialized it in the obvious way (i.e. exactly as above for single bits, but now utilizing a whole byte. Then, I combined the control bytes and the two input bytes into one array:\n",
      "  double input[SIZE];\n",
      "\n",
      "  int i;\n",
      "  for(i = 0; i < BYTE; i++){\n",
      "    input[i] = !!((a << i) & 128);\n",
      "    input[i+BYTE] = !!((b << i) & 128);\n",
      "    input[i+2*BYTE] = !!((c << i) & 128);\n",
      "  }\n",
      "Finally, I was able to run this through the sigmoid function:\n",
      " char out = 0;\n",
      "  for (i = 0 ; i< BYTE; i++){\n",
      "    double temp;  \n",
      "    temp = sigmoid(dot((double *)input, theta[i]));\n",
      "\n",
      "    if(temp>0.5){\n",
      "      out = out|(1<<(BYTE-i-1));\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return out ;\n",
      "Second Version\n",
      "I knew from before I had even started that hard wiring the code was not learning. I just wanted to make sure that everything was working correctly. The next step was to obviously have a neuron that could accept arbitrary weights as inputs along with the normal inputs and control bits.\n",
      "\n",
      "\"\"\"\n",
      "len(a.split())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "2064"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<u>Creativity</u>\n",
      "\n",
      "My project demonstrates creativity. I created an atificial neural network from scratch, implementing the details in C. While originally I had bold plans for my Artificial Neural Network, I was crushed by the daunting task, both because of the simple fact that the algorithms involved are highly complex and difficult to understand, but also because I chose to implement the program in C, a language that I have exposure to, but is also not my most comfortable language. \n",
      "\n",
      "My original plan had been to make my artificial Neural network be able to learn how to calculate floating point additions. On the surface, perhaps, this seems like a trivial task, as any modern computer can do floating point calculations. But in fact, this is quite an interesting and creative project for a nueral network to accomplish. As we all know, computers work in a world of ones and zeroes. Floating point calculations by their natures truncate, or approxiamte their answers. On top of that, as we will see in this paper, most beginning applications of Artificial Neural networks concentrate on simple boolean functions, and I thought it would have been interesting to try and up the game a bit. However, as we will also see, I couldn't do that and had to eventually drop back down to the simple boolean functions.\n",
      "\n",
      "The second reason why the floating point calculation is cool, besides for the fact that by their natures ANNs simply output a string of ones and zeros, is because the ANN is truelly learning how to do these calculations. This is like teaching a child how to multiply, but not only teaching him or her how to multiply single digit numbers, but arbitrary floating point numbers (actually in the system I had originally conceived, there still would have been limits on exactly how large the numbers could be). The way you teach a child to multiply is basically the same way you teach an ANN: practice, practice, practice.\n",
      "\n",
      "<u>Interesting Research Questions</u>\n",
      "\n",
      "(maybe this whole section is best at the very end)\n",
      "\n",
      "Unfurtunately the timing in the quarter system does not allow us to fully dive into a project. I would approximate that I spent something like 5 weeks programming the basic algorithms. Now that this is done, there are many questions that can be asked. \n",
      "\n",
      "Questions of topology. (at some point we should introduce what an ANN is)\n",
      "\n",
      "Questions of learning methods: one exampl at a time, batch, sleep, etc.\n",
      "\n",
      "Questions of memory: how many rules can an ANN of a certain topology handle. As one of my papers stated. For simple boolean, you don't need a hidden layer. For more complicated boolean operators, you will need to have a hidden layer. One hidden layer also works for addition. However, to multiply you will need to have two hidden layers. But on top of this is the memory we are already familiar with, where we can put information into short term memory for later and fast retrieval.\n",
      "\n",
      "chaining inputs and outputs so that's it can handle digits greater than one byte. I haven't figured out how to do this yet.\n",
      "\n",
      "\n",
      "sigmoid function dopeness. you have to do discrete calculus rather than normal calculus. It was easiest to implentmeant this as floating point, so for expediances sake I have not worked on this further. \n",
      "\n",
      "\n",
      "<u>C versus Python</u>\n",
      "\n",
      "C is an interesting choice because I can now, and as we will see in the future, I can get very close to the machine in order to get fast performance using a minimum of space. I did not choose Python because I wanted to implement a really efficient system. In retrospect this was probably a mistake. Given the complexity of the task, Python would have been a better choice for at least two reasons. One, Python has a way of making the complex seem, if not trivial, at least tackleble. Two, because of the aforementioned complexity, I was not actually able to implement the program in a clean and precise way, and in fact had to not even think about garbage collection and other issues of equal or perhaps more importance, as I will explain in a different section\n",
      "\n",
      "<u>description of an ANN</u>\n",
      "\n",
      "Artificial Neural Networks or ANNs is an old subfield of Arificial Intelligence research that has recently been resurgent. ANNs attempt to mimic the actual biological mechanisms of a brain. It began in 1943 when Warren McCulloch and Walter Pitts published a paper on them. The human brain is made up of billions of nuerons. Neurons are connected to each other by synapses. Each neuron has hundreds if not thousands of synapses as inputs and thence their output is also connected to thousands of synapses. And in theory, the neuron fires when certain stimuli from the various inputs reaches a certain threshold. The computer model tries to copy this behavior. Every neuron will have a certain number of inputs, and if a certain criterion is reached, then the neuron will fire and send its output to other neurons in the net. \n",
      "\n",
      "Rather than talking about all the steps necessary to create an ANN from the beginning. I am going to walk you hrough all the steps and design decisions that I made while implementing my Neural network. First, I will talk about simple perceptrons (I believe that's what they are called) and then..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<u>Walkthrough of my code</u>\n",
      "\n",
      "<u>First Version</u>\n",
      "\n",
      "First up, really before anything else is to talk about the good things about C. The really nice thing is how close it lets me get to the machine, and I actually operate at the individual level of bits and bytes. Unfourtunately, or really this is just the way that C is made, I can actually only operate at the level of Bytes. In order to manipulate the individual bits, I have to use 'masks'. Here below is a little procedure to ptrint the individual bits in a byte."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "#define BYTE 8\n",
      "\n",
      "void print_byte(char a){\n",
      "  int i;\n",
      "  for(i = 0; i < BYTE; i++) {\n",
      "      printf(\"%d\", !!((a << i) & 128));\n",
      "  }\n",
      "  printf(\"\\n\");\n",
      "}\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the get go, it should be pointed out that since we are going to consentrate our efforts on one byte at a time, we will be restricetd to positive numbers less than 256. To deal with bigger numbers we will have to do some type of chaining, I have not implemented this yet. \n",
      "\n",
      "My first efforts were directed towards the earliest research on ANNs and simply trying to express simple boolean functions like AND and OR. While later on, we wish to have the weights of the synapses vary according to their importance in a learned way, here, I was satisfied with hard-wiring the correct behaviour.\n",
      "\n",
      "Let's take a close look at how an AND perceptron would behave. There are two binary inputs into the perceptron, plus there is an ever present bias unit. This bias unit is sort of like a door or a gate, and only when the added weight of all the inputs bypass the weight of the door, does the perceptron fire. We are here assuming a hard cut-off (I will talk about this a little more momentarily). The bias unit is always set to one.\n",
      "\n",
      "$$if ~~~ \\sum {inputs * weights } \\ge 0 ~~~ then ~~FIRE!$$\n",
      "\n",
      "With two inputs and one bias unit, we need to have three weights. here are the weights that will make an AND perceptron work: 2 , 2, -3. Let's take a look at how that works. \n",
      "\n",
      "If both inputs are zero, then only the bias unit will contribute, and will contribute $0*2+0*2+1*(-3) = -3$ and therefore the perceptron will not fire. If the first input is a one, but the second input is zero, then the total contribution is $1*2+0*2+1*(-3)= -1$ and again the perceptron will not fire. If the first input is zero and the second input is one, the situation is almost exactly the same. Finally if both inputs are one we have  $1*2+1*2+1*(-3) = 1$ and finally the perceptron will fire, matching the exact performance of an AND gate.\n",
      "\n",
      "If you have never done so, I recommend trying to figure out the weights for an OR perceptron yourself.\n",
      "\n",
      "If you try to make the weghts for an XOR gate, you will quickly find that it is impossible. And that you will need to chain several perceptrons together. Before getting to this, however, I want to discuss my implementation of the AND gate in C, as I found that at every step in this journey led to decisions that implications for the life of the project.\n",
      "\n",
      "Obviously, in the beginning, and having played with someone else's python ANN. I was expecting that I could have arbitrary topologies or architectures. But my abilities with C would not allow that to happen.\n",
      "\n",
      "At this point in the project, I was still hoping to be able to do floating point arithmetic, so I needed to train my network to accept input much larger than just two bits plus a bias unit. At the time I hadn't set in stone that this would be my final architecture, but as I progressed, I realized that I would need to make decisions that would... and this was a decision that I made that stuck. What I decided for this is that I would accept a byte for each input and I would accept a final control byte, which would operate as my bias unit, but could be used in the future for some extra flexibility (I have not been able to take advantage of this.). I also decide to have only one byteo for the output (I now regret this, and would have much preferred haveing two bytes of output). This means that my perceptron would accept 24 bits of information and would output 8 bits. This makes for a total of 192 weights internally in each neuron or perceptron.\n",
      "\n",
      "Before dealing with that, we really must talk about the hard or soft squashing function. There are a few standard models. In the above example we used a hard squahing decision thing. But usually something with a softer edge has an advantage. The most common choice is the sigmoid or logistic function. I have actually had considerable dealings with this function in the past, but I will not burden the reader with that here and now. The sigmoid function is defined as \n",
      "\n",
      "$$\\phi (x) = \\frac 1 {1+ e^{-x}}$$\n",
      "\n",
      "Also, the derivative of this has the extremely simple form: $\\phi * (1 - \\phi)$ which will come into play when we discuss backpropagation.\n",
      "\n",
      "As I was saying we must sum over all the inputs multiplied by their weights. Two Bytes of input plus one byte for the control bit, gives us 24 bits to multiply together. I created a simple function for both of these two procedures:\n",
      "\n",
      "```\n",
      "#define SIZE 3*BYTE\n",
      "\n",
      "double sigmoid(double z){\n",
      "  if(z>4) return 1;\n",
      "  if(z<-4) return 0;\n",
      "\n",
      "  double denom = 1 + exp(-z);\n",
      "  return 1.0/denom;\n",
      "} \n",
      "\n",
      "double dot(double *a, double *b){\n",
      "  double out = 0;\n",
      "  int i;\n",
      "  for(i = 0; i < SIZE; i++){\n",
      "    out += a[i]*b[i];\n",
      "  }\n",
      "  return out;\n",
      "} \n",
      "```\n",
      "\n",
      "Setting up the AND neuron was fairly strightforward. The only crucial piece of technology was that I wanted to ensure that the control bit did indeed have a leading one, otherwise it couldn't be used as a bias. I did this witha simple mask:\n",
      "\n",
      "```\n",
      "a = a|128\n",
      "```\n",
      "128 being the byte that has a leading 1 with the rest of the digits zero. I then initialized a matrix:\n",
      "```\n",
      "double theta[BYTE][SIZE] = {};\n",
      "```\n",
      "and intitialized it in the obvious way (i.e. exactly as above for single bits, but now utilizing a whole byte. Then, I combined the control bytes and the two input bytes into one array:\n",
      "```\n",
      "  double input[SIZE];\n",
      "\n",
      "  int i;\n",
      "  for(i = 0; i < BYTE; i++){\n",
      "    input[i] = !!((a << i) & 128);\n",
      "    input[i+BYTE] = !!((b << i) & 128);\n",
      "    input[i+2*BYTE] = !!((c << i) & 128);\n",
      "  }\n",
      "```\n",
      "Finally, I was able to run this through the sigmoid function:\n",
      "\n",
      "```\n",
      " char out = 0;\n",
      "  for (i = 0 ; i< BYTE; i++){\n",
      "    double temp;  \n",
      "    temp = sigmoid(dot((double *)input, theta[i]));\n",
      "\n",
      "    if(temp>0.5){\n",
      "      out = out|(1<<(BYTE-i-1));\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return out ;\n",
      "```  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<u>Second Version</u>\n",
      "\n",
      "I knew from before I had even started that hard wiring the code was not learning. I just wanted to make sure that everything was working correctly. The next step was to obviously have a neuron that could accept arbitrary weights as inputs along with the normal inputs and control bits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}