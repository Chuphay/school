{
 "metadata": {
  "name": "",
  "signature": "sha256:560534fa2160ff30c10fcd46a5c47dd3492e231c5e91b99d6d4b34f0c6522166"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Graduate students are required to complete a research or design project in AI on a topic to be chosen in consultation with me. A written report on the project (minimum 2,500 words) and a brief oral presentation summarizing the same is expected at the end of the term. An ideal project should be one that demonstrates some creativity, attempts to answer some interesting research question(s), or offers an interesting AI solution to a problem of practical interest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = \"\"\"\n",
      "Where we keep it much like in the first version. Char a is simply the control bit we were talking about earlier. It is here for future flexibility, and so basically any char is okay, because again I will use an or command with 128 to make sure that the first bit is set to 1. We use this one as the bias. A good way to think of this bias is like an electrical ground. Char b and char c are the normal inputs that will be combined in someway, and as should be obvious from the declaration, one single char will be returned from this procedure. The internal guts are pretty much the same, but rather than having the weights hard-wired, we now accept a two dimensional matrix, which we call theta. Keep in mind that this two dimensional matrix has dimensions eight by twenty-four.\n",
      " \n",
      "\"\"\"\n",
      "len(a.split())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "147"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<u>Creativity</u>\n",
      "\n",
      "My project demonstrates creativity. I created an atificial neural network from scratch, implementing the details in C. While originally I had bold plans for my Artificial Neural Network, I was crushed by the daunting task, both because of the simple fact that the algorithms involved are highly complex and difficult to understand, but also because I chose to implement the program in C, a language that I have exposure to, but is also not my most comfortable language. \n",
      "\n",
      "My original plan had been to make my artificial Neural network be able to learn how to calculate floating point additions. On the surface, perhaps, this seems like a trivial task, as any modern computer can do floating point calculations. But in fact, this is quite an interesting and creative project for a nueral network to accomplish. As we all know, computers work in a world of ones and zeroes. Floating point calculations by their natures truncate, or approxiamte their answers. On top of that, as we will see in this paper, most beginning applications of Artificial Neural networks concentrate on simple boolean functions, and I thought it would have been interesting to try and up the game a bit. However, as we will also see, I couldn't do that and had to eventually drop back down to the simple boolean functions.\n",
      "\n",
      "The second reason why the floating point calculation is cool, besides for the fact that by their natures ANNs simply output a string of ones and zeros, is because the ANN is truelly learning how to do these calculations. This is like teaching a child how to multiply, but not only teaching him or her how to multiply single digit numbers, but arbitrary floating point numbers (actually in the system I had originally conceived, there still would have been limits on exactly how large the numbers could be). The way you teach a child to multiply is basically the same way you teach an ANN: practice, practice, practice.\n",
      "\n",
      "<u>Interesting Research Questions</u>\n",
      "\n",
      "(maybe this whole section is best at the very end)\n",
      "\n",
      "Unfurtunately the timing in the quarter system does not allow us to fully dive into a project. I would approximate that I spent something like 5 weeks programming the basic algorithms. Now that this is done, there are many questions that can be asked. \n",
      "\n",
      "Questions of topology. (at some point we should introduce what an ANN is)\n",
      "\n",
      "Questions of learning methods: one exampl at a time, batch, sleep, etc.\n",
      "\n",
      "Questions of memory: how many rules can an ANN of a certain topology handle. As one of my papers stated. For simple boolean, you don't need a hidden layer. For more complicated boolean operators, you will need to have a hidden layer. One hidden layer also works for addition. However, to multiply you will need to have two hidden layers. But on top of this is the memory we are already familiar with, where we can put information into short term memory for later and fast retrieval.\n",
      "\n",
      "chaining inputs and outputs so that's it can handle digits greater than one byte. I haven't figured out how to do this yet.\n",
      "\n",
      "\n",
      "sigmoid function dopeness. you have to do discrete calculus rather than normal calculus. It was easiest to implentmeant this as floating point, so for expediances sake I have not worked on this further. \n",
      "\n",
      "\n",
      "<u>C versus Python</u>\n",
      "\n",
      "C is an interesting choice because I can now, and as we will see in the future, I can get very close to the machine in order to get fast performance using a minimum of space. I did not choose Python because I wanted to implement a really efficient system. In retrospect this was probably a mistake. Given the complexity of the task, Python would have been a better choice for at least two reasons. One, Python has a way of making the complex seem, if not trivial, at least tackleble. Two, because of the aforementioned complexity, I was not actually able to implement the program in a clean and precise way, and in fact had to not even think about garbage collection and other issues of equal or perhaps more importance, as I will explain in a different section\n",
      "\n",
      "<u>description of an ANN</u>\n",
      "\n",
      "Artificial Neural Networks or ANNs is an old subfield of Arificial Intelligence research that has recently been resurgent. ANNs attempt to mimic the actual biological mechanisms of a brain. It began in 1943 when Warren McCulloch and Walter Pitts published a paper on them. The human brain is made up of billions of nuerons. Neurons are connected to each other by synapses. Each neuron has hundreds if not thousands of synapses as inputs and thence their output is also connected to thousands of synapses. And in theory, the neuron fires when certain stimuli from the various inputs reaches a certain threshold. The computer model tries to copy this behavior. Every neuron will have a certain number of inputs, and if a certain criterion is reached, then the neuron will fire and send its output to other neurons in the net. \n",
      "\n",
      "Rather than talking about all the steps necessary to create an ANN from the beginning. I am going to walk you hrough all the steps and design decisions that I made while implementing my Neural network. First, I will talk about simple perceptrons (I believe that's what they are called) and then..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<u>Walkthrough of my code</u>\n",
      "\n",
      "<u>First Version</u>\n",
      "\n",
      "First up, really before anything else is to talk about the good things about C. The really nice thing is how close it lets me get to the machine, and I actually operate at the individual level of bits and bytes. Unfourtunately, or really this is just the way that C is made, I can actually only operate at the level of Bytes. In order to manipulate the individual bits, I have to use 'masks'. Here below is a little procedure to ptrint the individual bits in a byte."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "#define BYTE 8\n",
      "\n",
      "void print_byte(char a){\n",
      "  int i;\n",
      "  for(i = 0; i < BYTE; i++) {\n",
      "      printf(\"%d\", !!((a << i) & 128));\n",
      "  }\n",
      "  printf(\"\\n\");\n",
      "}\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the get go, it should be pointed out that since we are going to consentrate our efforts on one byte at a time, we will be restricetd to positive numbers less than 256. To deal with bigger numbers we will have to do some type of chaining, I have not implemented this yet. \n",
      "\n",
      "My first efforts were directed towards the earliest research on ANNs and simply trying to express simple boolean functions like AND and OR. While later on, we wish to have the weights of the synapses vary according to their importance in a learned way, here, I was satisfied with hard-wiring the correct behaviour.\n",
      "\n",
      "Let's take a close look at how an AND perceptron would behave. There are two binary inputs into the perceptron, plus there is an ever present bias unit. This bias unit is sort of like a door or a gate, and only when the added weight of all the inputs bypass the weight of the door, does the perceptron fire. We are here assuming a hard cut-off (I will talk about this a little more momentarily). The bias unit is always set to one.\n",
      "\n",
      "$$if ~~~ \\sum {inputs * weights } \\ge 0 ~~~ then ~~FIRE!$$\n",
      "\n",
      "With two inputs and one bias unit, we need to have three weights. here are the weights that will make an AND perceptron work: 2 , 2, -3. Let's take a look at how that works. \n",
      "\n",
      "If both inputs are zero, then only the bias unit will contribute, and will contribute $0*2+0*2+1*(-3) = -3$ and therefore the perceptron will not fire. If the first input is a one, but the second input is zero, then the total contribution is $1*2+0*2+1*(-3)= -1$ and again the perceptron will not fire. If the first input is zero and the second input is one, the situation is almost exactly the same. Finally if both inputs are one we have  $1*2+1*2+1*(-3) = 1$ and finally the perceptron will fire, matching the exact performance of an AND gate.\n",
      "\n",
      "If you have never done so, I recommend trying to figure out the weights for an OR perceptron yourself.\n",
      "\n",
      "If you try to make the weghts for an XOR gate, you will quickly find that it is impossible. And that you will need to chain several perceptrons together. Before getting to this, however, I want to discuss my implementation of the AND gate in C, as I found that at every step in this journey led to decisions that implications for the life of the project.\n",
      "\n",
      "Obviously, in the beginning, and having played with someone else's python ANN. I was expecting that I could have arbitrary topologies or architectures. But my abilities with C would not allow that to happen.\n",
      "\n",
      "At this point in the project, I was still hoping to be able to do floating point arithmetic, so I needed to train my network to accept input much larger than just two bits plus a bias unit. At the time I hadn't set in stone that this would be my final architecture, but as I progressed, I realized that I would need to make decisions that would... and this was a decision that I made that stuck. What I decided for this is that I would accept a byte for each input and I would accept a final control byte, which would operate as my bias unit, but could be used in the future for some extra flexibility (I have not been able to take advantage of this.). I also decide to have only one byteo for the output (I now regret this, and would have much preferred haveing two bytes of output). This means that my perceptron would accept 24 bits of information and would output 8 bits. This makes for a total of 192 weights internally in each neuron or perceptron.\n",
      "\n",
      "Before dealing with that, we really must talk about the hard or soft squashing function. There are a few standard models. In the above example we used a hard squahing decision thing. But usually something with a softer edge has an advantage. The most common choice is the sigmoid or logistic function. I have actually had considerable dealings with this function in the past, but I will not burden the reader with that here and now. The sigmoid function is defined as \n",
      "\n",
      "$$\\phi (x) = \\frac 1 {1+ e^{-x}}$$\n",
      "\n",
      "Also, the derivative of this has the extremely simple form: $\\phi * (1 - \\phi)$ which will come into play when we discuss backpropagation.\n",
      "\n",
      "As I was saying we must sum over all the inputs multiplied by their weights. Two Bytes of input plus one byte for the control bit, gives us 24 bits to multiply together. I created a simple function for both of these two procedures:\n",
      "\n",
      "```\n",
      "#define SIZE 3*BYTE\n",
      "\n",
      "double sigmoid(double z){\n",
      "  if(z>4) return 1;\n",
      "  if(z<-4) return 0;\n",
      "\n",
      "  double denom = 1 + exp(-z);\n",
      "  return 1.0/denom;\n",
      "} \n",
      "\n",
      "double dot(double *a, double *b){\n",
      "  double out = 0;\n",
      "  int i;\n",
      "  for(i = 0; i < SIZE; i++){\n",
      "    out += a[i]*b[i];\n",
      "  }\n",
      "  return out;\n",
      "} \n",
      "```\n",
      "\n",
      "Setting up the AND neuron was fairly strightforward. The only crucial piece of technology was that I wanted to ensure that the control bit did indeed have a leading one, otherwise it couldn't be used as a bias. I did this witha simple mask:\n",
      "\n",
      "```\n",
      "a = a|128\n",
      "```\n",
      "128 being the byte that has a leading 1 with the rest of the digits zero. I then initialized a matrix:\n",
      "```\n",
      "double theta[BYTE][SIZE] = {};\n",
      "```\n",
      "and intitialized it in the obvious way (i.e. exactly as above for single bits, but now utilizing a whole byte. Then, I combined the control bytes and the two input bytes into one array:\n",
      "```\n",
      "  double input[SIZE];\n",
      "\n",
      "  int i;\n",
      "  for(i = 0; i < BYTE; i++){\n",
      "    input[i] = !!((a << i) & 128);\n",
      "    input[i+BYTE] = !!((b << i) & 128);\n",
      "    input[i+2*BYTE] = !!((c << i) & 128);\n",
      "  }\n",
      "```\n",
      "Finally, I was able to run this through the sigmoid function:\n",
      "\n",
      "```\n",
      " char out = 0;\n",
      "  for (i = 0 ; i< BYTE; i++){\n",
      "    double temp;  \n",
      "    temp = sigmoid(dot((double *)input, theta[i]));\n",
      "\n",
      "    if(temp>0.5){\n",
      "      out = out|(1<<(BYTE-i-1));\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return out ;\n",
      "```  \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<u>Second Version</u>\n",
      "\n",
      "I knew from before I had even started that hard wiring the code was not learning. I just wanted to make sure that everything was working correctly. The next step was to obviously have a neuron that could accept arbitrary weights as inputs along with the normal inputs and control bits. This is what the definition of the procedure looked like:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```\n",
      "char neuron(char a, char b, char c, double **theta);\n",
      "\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Where we keep it much like in the first version. Char a is simply the control bit we were talking about earlier. It is here for future flexibility, and so basically any char is okay, because again I will use an or command with 128 to make sure that the first bit is set to 1. We use this one as the bias. A good way to think of this bias is like an electrical ground. Char b and char c are the normal inputs that will be combined in someway, and as should be obvious from the declaration, one single char will be returned from this procedure. The internal guts are pretty much the same, but rather than having the weights hard-wired, we now accept a two dimensional matrix, which we call theta. Keep in mind that this two dimensional matrix has dimensions eight by twenty-four.\n",
      "\n",
      "Now that I had my neurons ready, it was time to start chaining them together to form a net. This was also where I started to have some intense problems with my design and my limited abilities to code in C. In doing this, two problems became apparent. I resolved these problems separetly over the course of weeks, so I will talk about these problems seperately, but I should remind you that I had both problems at the same time and fundamentally, these problems are intertwined.\n",
      "\n",
      "\n",
      "<u>The net </u>\n",
      "\n",
      "The main question, was how was I going to chain individual neurons together. I started off with this code:\n",
      "\n",
      "```\n",
      "typedef struct net{\n",
      "  int number;  \n",
      "  int length;\n",
      "  int *layers;\n",
      "  double *theta;\n",
      "} net;\n",
      "\n",
      "net *make_net(int number, int length, int *layers){\n",
      "\n",
      "  double *theta = malloc(SIZE*BYTE*sizeof(double));\n",
      "  net *out = malloc(sizeof(net));\n",
      "  \n",
      "  out->number = number\n",
      "  out->length = length;\n",
      "  out->layers = layers;\n",
      "  out->theta = theta;\n",
      "  return out;\n",
      "} \n",
      "\n",
      "void free_net(net *x){\n",
      "  //I never had the chance to implement this\n",
      "}\n",
      "```\n",
      "\n",
      "With a typical invocation looking like \n",
      "\n",
      "```\n",
      "  int layers[3] = {3,2,1};\n",
      "  net *my_net = make_net(6, 3,layers);\n",
      "  free_net(my_net);\n",
      "``` \n",
      "\n",
      "I added in some redundenc to the program. At the time I was bouncing between two representations. One used the length of the network, in this example, the length is three. The other representation was interested in the number of neurons in the net, in this example there are 6 (or is itonly 3?). Obviously, these are dependent on each other, and with the help of the array layers, you can figure one quantity from the other. Truthfully length (or size) is reduntant, but it just seems easier to pass this extra information. In a future version, I will suppress one in the favor of the other, but for now, the extra information is welcome.\n",
      "\n",
      "Looking back with hindsight, I can see that I was definately on the right track, hwever I got befuddled, and I chose a different track. But this was the track that I should have chosen. Two problems should become apparent while looking at this. The first is to look at how I invoked the layers. My topology is a 3X2X1 network. In theory this should be fine, but if you actually think about what my neurons are capable of, i.e. each neuron accepts 2 inputs and outputs one output, that this is topology is not acceptable at the present moment. It needs either fuller specification of exactly how the wiring between the nodes will be accomplished, or the individual neurons themselves will have to be made more flexible. It took me several weeks of playing around before I finally came upon the solution that I would stick with. Because of the shortness of time, I decided I needed to keep the problem as simple as possible, while still keeping all the main ingrediants. What this meant was that I had to limit what my net was going to be able to accomplish. I had to limit my net because I needed to finish this project, so that I could turn it in in time.\n",
      "\n",
      "The way I decided to limit my net, was by making the architecture or topology quite restricted. What I decided was that the only acceptable topology was to be of the form 2X2X2X...X2X1. Here are my original comments when I made this decision:\n",
      "```\n",
      "  //here's what I'm doing from now on\n",
      "  //Im going to assume that the net is of the form\n",
      "  //2X2X2...2X2X1\n",
      "  //this is C\n",
      "  //and I'm no expert. \n",
      "  //And most importantly,\n",
      "  //Let's make sure we can finish this project!!\n",
      "```  \n",
      "\n",
      "This is obviously severly limiting, and I hope that in a future version of this program, I will be able to relax this requirement. However, even with this topology, we should still be able to do some fun experiments with then net, however floating point calculations are not going to be possible, because each floating point number requires at a minimum about 2 bytes of information, and in order to add or multiply, you would therefore need to be able to input about 4 bytes into the net. My net would only accomadate 2 bytes of iinformation, and was thus unsuitable for floating point arithmatic. \n",
      "\n",
      "<u> Three Dimensional Matrices </u>\n",
      "\n",
      "However, at the time the final architecture was not even the most pressing problem. I don't know if you can see it, but there is a bug in my make_net proceudre. The problem is in this line:\n",
      "\n",
      "```\n",
      "double *theta = malloc(SIZE*BYTE*sizeof(double));\n",
      "\n",
      "``` \n",
      "\n",
      "In retrospect, I can see how I could have done this better... but not now. The problem is that each neuron accepts a two dimensional theta matrix in order to calculate its output. Therefore, in a net with, say, 10 neurons, you will need a three dimensional matrix with dimensions 10X32X8. C does not have an efficient way to make three dimensional matrices, and we must program them by hand. Now, there is a good way to do this, a bad way to do this, and the way I did it (which I guess is the ugly way to do it). Let me detail each way.\n",
      "\n",
      "First up is the good way to do it. The way you really should do this is by laying all the values in a single array and then using a smart algorithm or ordering principle to gain access to each element in the array. A typical implentation of this might look like:\n",
      "\n",
      "```\n",
      "int index(int x, int y, int z) {\n",
      "  return x + (y*xSize) + (z*ySize*xSize);\n",
      "}\n",
      "\n",
      "double value = array[index(a, b, c)];\n",
      "```\n",
      "\n",
      "Not only is this the smart way to do this, you can also imagiine that performing operations on this would be fast and efficient. All the memory would be allocated in one big chunk of memory.\n",
      "\n",
      "The bad way to do this would be to actually implement a three dimensional matrix in C, in much the same way that you would do a two dimensional matrix. This would necessitate three seperate calls to malloc, and therefore your data would be spread out all over the RAM, and you would expect slower performance Also because the implementation is more complicated, it would be much easier to introduce subtl bugs into your program.\n",
      "\n",
      "Finally is the way that I implemented it, which is purely an ugly hack that barely gets the job done. Parenthetically, if I ever get a chance to work on this neural net again, this will be the first thing I will want to change. Here is what it looks like:\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}