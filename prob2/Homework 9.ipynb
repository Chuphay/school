{
 "metadata": {
  "name": "",
  "signature": "sha256:707ca1e48915864e58c970219cb08eeae50329857f7e6e0574f3529421da8858"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Homework #9"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>6.7-1.</h3> <h4>Let $X_1 , X_2 , . . . , X_n$ be a random sample from\n",
      "$N(0, \\sigma^2 )$.\n",
      "\n",
      "(a) Find a sufficient statistic Y for $\\sigma^2$ .</h4>\n",
      "\n",
      "We can write the joint distribution in the following form:\n",
      "\n",
      "$$f(x_1,...,x_n;\\theta) = (2 \\pi \\theta)^{-n/2}exp\\left[- \\frac 1 {2 \\theta} \\sum x_i^2\\right]$$\n",
      "\n",
      "therefore, $Y = \\sum x_i^2$ is a sufficient statistic for $\\theta$\n",
      "\n",
      "<h4>(b) Show that the maximum likelihood estimator for $\\sigma^2$ is\n",
      "a function of Y.</h4>\n",
      "\n",
      "We write the likelihood function as \n",
      "\n",
      "$$L(\\theta) = (2 \\pi \\theta)^{-n/2}exp\\left[- \\frac 1 {2 \\theta} \\sum x_i^2\\right]$$\n",
      "\n",
      "Taking the log gives\n",
      "\n",
      "$$log\\left(L(\\theta)\\right) = -\\frac n 2 log(2 \\pi \\theta)- \\frac 1 {2 \\theta} \\sum x_i^2$$\n",
      "\n",
      "Taking the derivative and setting it equal to zero yields\n",
      "\n",
      "$$\\theta = \\frac 1 n \\sum x_i^2$$\n",
      "\n",
      "which is obviously a function of Y\n",
      "\n",
      "<h4>(c) Is the maximum likelihood estimator for $\\sigma^2$ unbiased?</h4>\n",
      "\n",
      "???"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>6.7-2.</h3> <h4>Let $X_1 , X_2 , . . . , X_n$  be a random sample from a\n",
      "Poisson distribution with mean \u03bb > 0. Find the conditional probability $\\mathbb{P}(X_1 =x_1, . . . , X_n = x_n | Y = y)$,\n",
      "where $Y = X_1 , X_2 , . . . , X_n$ and the nonnegative integers\n",
      "$x_1 , x_2 , . . . , x_n$ sum to y. Note that this probability does not\n",
      "depend on \u03bb.</h4>\n",
      "\n",
      "According to the book, \"Unless the sum of the nonnegative integers $x_1 , x_2 , . . . , x_n$ equals y, this conditional probability is <b>obviously</b> equal to zero, which does not depend on p. Hence, it is interesting to\n",
      "consider the solution only when $y = x_1 + \u00b7 \u00b7 \u00b7 + x_n$ . From the definition of conditional\n",
      "probability, we have\"\n",
      "\n",
      "$$\\mathbb{P}(X_1 =x_1, . . . , X_n = x_n | Y = y) = \\frac {\\mathbb{P}(X_1 =x_1, . . . , X_n = x_n )}{\\mathbb{P}(Y=y)}$$\n",
      "\n",
      "We also recall, from a long time ago, that the sum $Y = X_1 +...+ X_n$ has the Poisson distribution with parameter $n\u03bb$. Therefore,\n",
      "\n",
      "$$\\mathbb{P}(X_1 =x_1, . . . , X_n = x_n | Y = y) = \\frac {\\lambda^{\\sum x_i} e^{-n \\lambda}}{\\lambda^{\\sum x_i} e^{-n \\lambda}}  \\frac {(x_1 + ...+x_n)!}{x_1! ... x_n!} = \\frac {(x_1 + ...+x_n)!}{x_1! ... x_n!}$$\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>6.7-4.</h3> <h4>Let $X_1 , X_2 , . . . , X_n$ be a random sample from a distribution with pdf $f(x;\\theta) = \\theta x^{ \\theta \u22121 }$, 0 < x < 1, where\n",
      "0 < \u03b8 .\n",
      "\n",
      "(a) Find a sufficient statistic Y for \u03b8.</h4>\n",
      "\n",
      "We can write the joint pdf in the exponential form \n",
      "\n",
      "$$f(x_1,...,x_n;\\theta) = exp \\left[n log \\theta + (\\theta - 1) log \\sum x_i \\right]$$\n",
      "\n",
      "Therefore, $log \\sum x_i$ is a sufficient statistic.\n",
      "\n",
      "\n",
      "<h4>(b) Show that the maximum likelihood estimator $\\hat \\theta$ is a\n",
      "function of Y.</h4>\n",
      "\n",
      "We write the likelihood function as \n",
      "\n",
      "$$L(\\theta) = \\theta^n \\sum x_i^{\\theta -1}$$\n",
      "\n",
      "Taking the log gives\n",
      "\n",
      "$$log\\left(L(\\theta)\\right) = n log(\\theta)+  (\\theta -1) log( \\sum x_i)$$\n",
      "\n",
      "Taking the derivative and setting it equal to zero yields\n",
      "\n",
      "$$\\hat \\theta = \\frac n {log( \\sum x_i)}$$\n",
      "\n",
      "which is obviously a function of Y\n",
      "\n",
      "<h4>(c) Argue that $\\hat \\theta$  is also sufficient for \u03b8 .</h4>\n",
      "\n",
      "erm... since $\\hat \\theta$ is simply a functions of the sufficient statistic from part a, it is clearly also sufficient."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>6.7-7.</h3> <h4>Let $X_1,X_2,...,x_n$ be a random sample from the\n",
      "distribution with pmf $f (x; p) = p(1 \u2212 p)^{x\u22121}$ , x = 1, 2, 3, . . .,\n",
      "where 0 < p \u2264 1.\n",
      "\n",
      "(a) Show that $Y = \\Sigma^n_{i=1} X_i$ is a sufficient statistic for p.</h4>\n",
      "\n",
      "The joint distribution is given by $p^n(1-p)^{\\sum x_i -n}$ therefore, Y is sufficient.\n",
      "\n",
      "<h4>(b) Find a function of Y that is an unbiased estimator of \u03b8 = 1/p.</h4>\n",
      "\n",
      "??? I really don't get these unbiased estimators???\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>6.7-10.</h3> <h4>Find a sufficient statistic for \u03b8 , given a random sample, $X_1 , X_2 , . . . , X_n$ , from a distribution with pdf\n",
      "$f (x; \u03b8 ) = \\{\\Gamma (2\\theta )/[ \\Gamma(\u03b8 )]^2 \\}x^{\u03b8 \u22121} (1 \u2212 x)^{ \u03b8 \u22121} $, 0 < x < 1.</h4>\n",
      "\n",
      "Man... I'm not sure if I'm getting these at all, because it seems totally clear that $\\sum x_i$ is sufficient."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}